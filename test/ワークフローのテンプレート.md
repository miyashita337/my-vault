

# 開発ワークフロー
基本以下のようなワークフローでいつも開発してます、認識の相違があれば、気軽に指摘してほしい（いや実は全然テストなんて作ってませんでしたとか、動作検証はしてないとか、テストは〜〜しかやってないなど）
### ①PROGRESS_TRACKER.mdをみて、優先度の高いものからやる
プレイヤー：ClaudeCode
タスク粒度は一つづつやる

### ②実装前 テストを作る
プレイヤー：ClaudeCode
* * 対応内容を把握してテストファーストでテストを作る
* 基本的に機能を追加されるたびにテストも追加される
* バグが出るたびにテストを作る
* そのため、テスト数は多いいが、全部のテストを通すことはソフトウェアの品質の担保に繋がる（５〜１０分程度のテストは許容範囲、それ以上テストの時間がかかるなら、一旦ユーザーにヒアリングして方針を確認する）

### ③実装
プレイヤー：ClaudeCode
①のタスクを実装

### ④実装後
プレイヤー：ClaudeCode
* 動作検証をする検証方法は画像抽出して画像が出力されるまで*
* 動作検証後、テストを実行、ここでいうテストはUnitテスト。毎回やる
	* テストを実行、エラーなら実装に戻ってエラーがなくなるまで修正
	* 

### ⑤gpt-4oもしくはGeminiにキャラクター抽出ができてるか評価してもらう
プレイヤー：ClaudeCode＋gpt-4o(Gemini)
* これはOPTIONAL扱い、各それぞれのもデリには制限があるからLimit制限が来たら、エラーを出すが、エラーが出たとしてもそのまま続行する（なぜなら最終的には人間に夜評価があるから、AIによる評価はまだ努力目標レベル）

### ⑦人間の視覚で評価
プレイヤー：人間、ユーザー
評価内容が悪かったら①に最優先事項で追記してもう一度最初からワークフローを実行してもらう

### ⑧release
プレイヤー：ClaudeCode
claude カスタムスラッシュコマンドである　/relaaseコマンドでバージョンあげる



# 質問

* このワークフローを[Claude Code GitHub Actions](https://docs.anthropic.com/ja/docs/claude-code/github-actions)に置き換えることは可能か？
* いまCIを入れてないですが、入れることはできますか？
	* 多分いまユニットテストの実装があると思いますが、githuｂでも
* どこからの起点か？
	* ⑦人間の視覚で評価でISSUEや
	* ①PROGRESS_TRACKER.mdをみて、優先度の高いものからやるでISSUEを作ってそこからGithubActionを回すというかんじでしょうか？

## 不明点

不明点あればヒアリングして
　おそらくユーザーも CLaudeCodeGithubAction がどのように今までのワークフローから肩代わりができるかも想像がついてないと思います、
　たとえば「現状のワークフローの〜〜の部分をCLaudeCodeGithubActionのXXXの部分に置き換えれる」などの提案も含めておねがいしたい
不明点をすべて洗い出したら
PROGRESS_TRACKER.mdに追記して実行してほしい



# ヒアリングの答え

  ❓ ヒアリング事項

>  1. GitHub Organization/Repository
>    - 現在のforkリポジトリで作業を続けますか？
>    - 独自のプライベートリポジトリに移行しますか？

ここまでくると、独自プライベートリポジトリも考慮したいのですが、プライベートリポジトリにするメリット・デメリットがまだわからないので引き続き教えてほしい

>  2. API制限への対応
>    - GPT-4O/Gemini評価の頻度はどの程度必要ですか？
>    - コスト面での制約はありますか？

GPT-4Oは今GPT Plusプランです、限定的に使えるなら使いたいレベルというのと、画像生成や画像抽出のレビューや、画像抽出画像生成のアプローチがメイン
GeminiはまだFreeプラン
なのであくまで「使えたら」レベルなのでLimit制限がかかったらスルーしてほしい


>  3. 既存資産の移行
>    - PROGRESS_TRACKER.mdの内容をIssue化しますか？

最終的には移行しますが、１〜２日使ってみてダメだったら戻したいので一旦PROGRESS_TRACKER.mdは残しておいてください


>    - 既存テストをGitHub Actionsに統合しますか？

最終的には統合したい。１〜２日使ってみてダメだったら戻したいので一旦コピーと言う形にしておいてほしい


>  4. 開発フローの優先順位
>    - すべてを一度に移行？それとも段階的？

段階的、移行に失敗する、リスクもあるので一気にはこわい


>    - 最も自動化したい部分はどこですか？

①〜④

>  5. チーム構成
    - 現在は一人開発ですか？
    - 将来的にチーム開発の予定は？

ふえません、基本一人開発



--------------------------
 📚 作成したテンプレート



# ワークフローレビュー

## このシステムで利用するソフトウェアハードウェアの仕様
spec専用のmdファイルを作ってほしい
今はbatch_extraction_template.mdに一部まとまってますが、ちょっと足りてないのと他でも参照しているので一つにまとめたほうがいい
* ファイル名は：segment-anything/spec.md
	* つまりルート直下
* これらは、実装（MergeRequest, Pullequestなどの修正）の段階で随時更新すること、ユーザーである人間がMergeボタンを押しタイミングでこのspec.mdは更新される仕組みがほしい(githubのhooksとかでできないかな？)

内容は以下

```bash
# 必須ソフトウェア
- Python 3.8+
- CUDA対応GPU（推奨: 8GB VRAM以上）
- segment-anything v0.4.0
- ultralytics (YOLOv8)  
# 必須モデルファイル
- sam_vit_h_4b8939.pth (SAM ViT-H)
- yolov8x6_animeface.pt (アニメ特化YOLO)
```

適宜何かしら仕様変更によりspecが追加削除修正があるたびにここが変更されること
(localhostのUnitTestに追加して、一番最初にチェックしたほうがいい、どういうチェックかというとこのspec.mdの内容が実際の環境と一致してるかのチェックすること)
よくあるのが、バージョンが一致してないなど


##   1. バッチ抽出ワークフローテンプレート
  - ファイル: docs/workflows/batch_extraction_template.md
>  - 内容: kaname08の実行経験をベースにした標準化手順
  - 特徴: 環境準備から結果確認まで完全な手順書

* まずkanameなどの特定名などを実装や公開するmdファイルには記載しないこと
	* input path , output path で使われる分には問題ない
		* 逆にそれ以外でのネーミングの仕様を禁止する
		* （背景）あくまでそれはテストで利用しているファイルパスの一部である
	* いわゆるDatasetNameの部分は動的に変わるので今日kanameという名前だったとしても来週別の名前になる可能性が高い
	* kaname~.pyを実行やkaname*.mdを参照というようなネーミングは辞めること

* 「環境要件」は
`../../spec.md を参照`
という形にして一元管理すること

> ls -la "$INPUT_DIR" | grep -E '\.(jpg|png)$' | wc -**l**

画像は「jpg」「png」「webp」の三種類とする


  
##   2. 進捗追跡テンプレート
  >- ファイル: docs/workflows/progress_tracking_template.md
  - 内容: リアルタイム進捗管理の構造化テンプレート
 > - 特徴: 自動化スクリプト連携機能付き

```
>- **バージョン**: segment-anything v0.4.0
>- **処理エンジン**: SAM + YOLOv8
```
バージョンや処理エンジンなども  `../../spec.md を参照` 

##  3. 品質評価ガイド
>  - ファイル: docs/workflows/quality_evaluation_guide.md
  - 内容: A-F評価基準と詳細チェックポイント
>  - 特徴: 手動・自動評価の両方に対応

内容に問題なし


>  4. トラブルシューティングガイド
  - ファイル: docs/workflows/troubleshooting_guide.md
>  - 内容: 今回解決した問題を含む包括的な対処法
>  - 特徴: 4段階の問題分類と緊急時対応
#### 追加する

```
Command 'python' not found, did you mean:
```
これはたんにpythonじゃなくてpython3コマンドで実行すれば回避できる

UnitTestなどでよく出るエラーも、随時ここに追記しておくこと

なにか不明点あれば実装前にヒアリングして



---------------



>  # 環境整合性チェック実行
  python3 tests/test_environment_spec.py
>
  # クイックチェック
>  python3 tests/test_environment_spec.py --quick
  # 新しいバッチ処理実行時
>  # 1. spec.mdで要件確認
  # 2. batch_extraction_template.mdの手順に従う
>  # 3. 自動的にGitHub Actionsでチェック実行
>これで将来のプロジェクト拡張に対応できる堅牢な基盤が確立されました！


よくわからないのですが、ユーザーからの異大雑把な指示で「バッチ実行して、inputは〜〜、outputは〜〜、品質は最高品質で」
というような大雑把な依頼でも
* 環境整合性チェック実行
* クイックチェック
* spec.mdで要件確認
みたいに統一された動きをしてくれるという認識でいいですか？


------------


ワークフローに関してですが、もうちょっと詰めたい。ちゃんとしたい


以下の内容が人間とAIが最善策で自動的にPDCAを回すワークフローナノではないかと思ってるのでこれ docs/workflows/以下にドキュメントとして残してほしい


```

### ①　ISSUE作り

プレイヤー：人間＋ClaudeCode+GPT
ISSUEを作ってGithubのISSUEに登録

#### ISSUE作り
* 人間からの評価、フィードバックをISSUEにする
* 人間が発案、要件定義、フィードバックををだす、その内容を「ClaudeCode」と「gpt-4o」が協議して、ベストプラクティスを出す（gpt-4oがLimit制限でエラーの場合はGeminiや他のモデルに依頼）
* この競技自体がとてもtokenを食う可能性を考慮して議事録はつけておくこと、途中でLimit制限がかかっても他のモデルが途中参加しても大丈夫なように議事録をつけておく
* 人間からの評価やフィードバックは曖昧だったりするので、それをClaudeやgpt-4oが言語化、具体化をする
* 提案内容はメリット・デメリット、工数を算出する
  * ClaudeやGPTがいくつか素案を提示した後、人間が採決する
  * 人間からLGTMをもらったらない右葉をPROGRESS_TRACKER.mdに追記（長い内容になるなら、別ファイルに分けて、参照してもらうようにする）
* 依頼内容がもし工数大、規模が大きいの場合はISSUEの作り方も考慮してサブフォルダ、サブISSUEなど作って体系的にわかりやすくすること

#### PROGRESS_TRACKER.mdをみて、優先度の高いものからやる
タスク粒度は一つづつやる
ISSUEにするときは、英語に直してユーザーにコピペしやすいように出力させる

### ②実装前 テストを作る
プレイヤー：GithubAction+ClaudeCode
* * 対応内容を把握してテストファーストでテストを作る
* 基本的に機能を追加されるたびにテストも追加される
* バグが出るたびにテストを作る

### ③実装
プレイヤー：GithubAction+ClaudeCode
①のタスクを実装

### ④実装後テスト
プレイヤー：GithubAction+ClaudeCode
* 動作検証をする検証方法は画像抽出して画像が出力されるまで*
* 動作検証後、テストを実行、ここでいうテストはUnitテスト。毎回やる
	* テストを実行、エラーなら実装に戻ってエラーがなくなるまで修正
	* 

### ⑤MR
プレイヤー：GithubAction+ClaudeCode
ここでMerrgeRequest,PullReqeustをだす

### ⑥ windows機でpullしてlocalでテストバッチ
プレイヤー：ClaudeCode(local)＋人間
⑤のMRをpullしてそのMRのブランチでバッチを出力(localhostのWindows機が出力)する、
人間は
  * pullしてブランチ変更
  * input path, output pathを決める
これだけをする

※基本バッチ処理はバックグラウンドで行うこと
完了したら、Pushoverにて人間のスマホに通知する

いったんここの時点で何個の抽出が成功したかの判定ができる
* 判断基準
  * 出力が90%以上成功していること
例）inputのディレクトリ内にある画像が100個だった場合
90個以上抽出された画像が出力されてること


### ⑦評価
#### 人間の目視による評価
プレイヤー：人間
ホントの意味での品質評価はここで決まる
* 判断基準
  * 出力されたすべて画像の評価がB判定であること、B評価が全体の50%以上であること
例）90個以上抽出された画像だった場合
90個内の50%であるから45個以上がB判定であること
※AIの評価が楽観的すぎるので、評価基準が厳し目になっているが、この判定は随時変わる。ある程度目処がついたら次の段階へすすむこともある

##### 重要：差し替えし
評価内容が悪かったら①に最優先事項で追記してもう一度最初からワークフローを実行してもらう


#### gpt-4oもしくはGeminiでの自動評価
プレイヤー：ClaudeCode＋gpt-4o(Gemini)
* これはOPTIONAL扱い、各それぞれのモデルには制限があるからLimit制限が来たら、エラーを出すが、エラーが出たとしてもそのまま続行する（なぜなら最終的には人間による評価があるから、AIによる評価はまだ努力目標レベル）
* それでもLimitでこれ以上評価できない場合はエラーにして先へ進むこと（あくまで努力目標なのでエラーなら次へ進む）

### ⑧マージ
プレイヤー：人間
⑦の評価が成功した場合は人間がマージボタンをおしてマージする

### ⑦localのmerge & release
プレイヤー：ClaudeCode
* マージされたのでmasterにchekcout後、git pullして最新を取り込む
* claude カスタムスラッシュコマンドである　/relaaseコマンドでバージョンあげる

```

すでにある
batch_extraction_template.md
progress_tracking_template.md
quality_ealuation_template.md
trubleshooting_guide.md
なども良い内容なので、うまい具合に情報が重複しないように両立してほしい



-----------

docs/workflows/ai_human_collaboration_workflow.mdですが
ブラウザから閲覧で
docs/workflows/ 直下に来たときに、README.mdみたいに、自動的に読み込まれるようにしたいのですが、そういう場合てREADME.mdにしないとだめですか？あまり同名のREADME.mdというのを作りたくないのですが