
いまバッチで一括に複数画像の抽出を依頼します
* *今いまバッチで一括に複数画像の抽出できると思うのですが

# 質問
* 抽出成功の可否の自己評価はどういう仕組で評価してるのでしょうか？


いつのまにか、claude-screenshotの /ss claude カスタムコマンドが消えてます
もう一度インストールし直してください
git clone git@github.com:miyashita337/claude-screenshot.git


# 推察

評価の方法（SAM）は基本的にこの図解の形だと思います
相違ないですか？

![[Pasted image 20250716200942.png]]




  - 図解では単純な「マスク出力」ですが、実際は5つの品質評価手法で最適結果を選択
  - balanced, confidence_priority, size_priority, fullbody_priority, central_priority
この５つの品質評価手法で最終的に出力するのはどういうパターン、アルゴリズムでしょうか？
それとも人間が評価した内容のスコアが高い順から選んでるということでしょうか？

＞  コードを確認した結果、アルゴリズムベースの自動選択が行われています。人間の評価スコアを参考にしていますが、実際の選択は自動化されています。

その方向性は正しい

質問なんですが、今後もっと精度を上げる場合、どちらがいいでしょうか？
* 人間の評価システムから手動で評価してそれを参考に学習させる(精度としては、これが一番よし、ただしコスト大)*
* システムが、自動抽出 -> 自動評価をすすめて学習させる(人的コストは最小、ただし精度が不明) 
* コストパフォーマンスと精度のトレードオフになるとは思うのだが、最適解と具体的な工数を弾き出してみて


＿
# 追加質問
* Phase1にした場合、マシンの挙動はカクつきますか？
* 例えばの話ですが、Phase1の状態の自動抽出 -> 自動評価をバックグランドで実行しつつ、表では別の画像抽出を試してみるというかんじ
	* もっと具体的に言うと
		* Phase1の状態の自動抽出 -> 自動評価　これは別ブランチレベルで作ってそれを運用
		* これまでの人間による手動での評価をしてみて、どちらが実際に抽出の精度が高いかを検証してみたい

これはポジティブに返答するではなく、過去のハングアップ事情を考慮して慎重に返答してください


Phase1に実行してほしいのですが
バイブコーディングをしてて懸念したのが以下です
*~/.claude/CLAUDE.md　を前提に依頼してるが、信頼に値する挙動をしてない
何かしら矛盾を起こして、こちらの要件とは違った方法をしでかすことがある
そういうときって、同指示するのが適切なのかがわからない
抽象的すぎて申し訳ないが一ヶ月以上Claudeに接しての感想です
これをよりベターにするにはどうすればいいでしょうか？
CLAUDE.mdがまずいのか、その他のmdファイルが余計なことをしてるのか

不明なところいっぱいあると思うので一旦ヒアリングしてほしい


>  - 質問: 「こちらの要件とは違った方法をしでかす」具体例を2-3個教えてください
  - 例: 「Aを実装して」と言ったのに、勝手にBも実装した、など

①例えば先日インストールした/mnt/AItools/claude-screenshot の /ssこまんどですが、先程の会話で分かる通り、いつの間にか破綻してる、もしくはいつの間にかアンインストールされてる

②そもそもキャラクター抽出が人間の評価と乖離してる
これはもっと学習コストをあげさせるしかないですかね？

③何度か「ハングアップしないでね」というふうに依頼してるのに、実際画像処理させたらハングアップしたりとか

④全体的にスクリプトが作りっぱなしで整理整頓もされてない、これは私の指示の問題かも
　もう少し、featureディレクトりにして機能ごとにフォルダを分けてもいいかも
　https://zenn.dev/michiharu/articles/c2794ddbfec05b

⑤短期目標（〜を作る）などはできても
　中期目標（戦略的に画像抽出評価が低コストで上がる仕組みを作る）
　長期目標（ユーザーが求めてるがぞう抽出ができてる）
　ができないこと


>  B. 矛盾する挙動

>  - 質問: 同じ指示を出したのに、前回と今回で異なる実装をしたケースはありますか？
  - 質問: CLAUDE.mdの指示を無視して、デフォルト挙動に戻ったケースはありますか？

たとえば、「なにか実装する前に先にテストを作成」みたいな文章がありますが作ってますでしょうか？
毎回作られてるならすいません、私の確認ミスです
ここまで実装してるなら、テストで矛盾を起こさないように実装ー＞テストー＞実装を繰り返すのが通常のエンジニアですが、それが見当たらない、それはAIだから自動的に全部見越して、全部正解を作ってるからでしょうか？それとも指示を忘れて実際はテストしてないのでしょうか？

Claudeが作って「できた」という報告で「動作できなかった」
